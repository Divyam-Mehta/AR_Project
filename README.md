# AR-Based Onion Quality Inspection & Sorting System

## ğŸ¥ Test Video

<p align="center">
  <img src="media/AR_Real_Farm_1.gif" width="640" />
</p>

This repository contains the code for an **Augmented Realityâ€“based onion inspection system** developed at the **THINC Lab, University of Georgia**.

The system:

- Tracks onions moving on a **high-speed conveyor** using fused data from **two standard webcams**.
- Uses a **YOLO-v8 model** (trained on 8000+ images) to detect onions and classify them as *good* or *blemished*.
- Uses a **custom slope- and/or velocity-based tracker** to estimate onion motion in real time.
- Projects **red (bad) or green (good) indicators** directly onto each onion using a **short-throw projector**, creating an AR overlay synchronized with the conveyor.

---

## âœ¨ Key Features

- **Dual-camera fusion** (anterior & posterior views).
- **YOLO-v8â€“based blemish detection** using a custom-trained model.
- **Multiple tracking backends**:
  - `VelocityTracker` (default, best for high-speed belts),
  - `SlopeTracker`,
  - `CentroidTracker`.
- **Real-time AR projection** using Pygame + short-throw projector.
- **Coordinate mapping** from camera pixels â†’ real-world mm â†’ projector pixels.
- **Automatic dataset logging** of good/bad onions for offline analysis.

---

## ğŸ“ Project Structure

From the repository root:

```text
AR_Project/
â””â”€â”€ THINC_code/
    â”œâ”€â”€ CentroidTracker.py       # Baseline centroid-only tracker
    â”œâ”€â”€ SlopeTracker.py          # Tracker using trajectory slope info
    â”œâ”€â”€ vel_tracker.py           # Velocity-based tracker (default)
    â”œâ”€â”€ deep_sort.py             # Optional tracker (in development)
    â”œâ”€â”€ iou.py                   # IoU utilities for overlapping detections
    â”œâ”€â”€ functions.py             # Sorting, DB operations, helper utilities
    â”œâ”€â”€ mapping.py               # Camera â†” world â†” projector mapping logic
    â”œâ”€â”€ vars.py                  # Global constants & configuration
    â”œâ”€â”€ onion.py                 # Onion-related data structures (if used)
    â”œâ”€â”€ test_yolo.py             # YOLO model sanity check script
    â”œâ”€â”€ rough.py                 # Experimental / sandbox code
    â”œâ”€â”€ bytetrack.yaml           # Tracker config (if using ByteTrack-style logic)
    â”œâ”€â”€ main.py                  # ğŸ”¥ Main runtime: detection + tracking + AR
```

---

## ğŸ” Processing Pipeline

Steps:

Frame Capture
main.py opens two webcams:
- Anterior camera: onion entry side
- Posterior camera: onion exit side

Pre-processing
- Rotate frames (90Â° clockwise).
- Crop to conveyor area.
- Resize to a common resolution (RESIZE_RES from vars.py).

YOLO-v8 Detection
- Run inference on the fused frame.
- Filter detections by class and confidence.
- Remove overlapping onions using IoU checks (iou.py).

Tracking
- Pass bounding boxes to VelocityTracker (or other tracker).
- Maintain an objectID â†’ centroid mapping.
- Print/log ID, centroid, and time difference between frames.

Classification & ID Association
- Match YOLO detections to tracker IDs using centroid proximity.
- Store final class (good/bad) in id_class[objectId].

Coordinate Mapping
- Convert camera pixel coordinates â†’ world coordinates in mm (pixelToMM).
- Project world coordinates â†’ projector pixel coordinates (mmToPixelProjector).

AR Projection
Use Pygame to draw:
- ğŸŸ¢ Green circle for good onions.
- ğŸ”´ Red circle + white cross for blemished onions.
- Mirror/flip the Pygame surface to align with projector.

Database & Sorting
Use functions like anterior_check, posterior_check, good_or_bad, and sort() from functions.py to:
- Detect when an onion passes anterior/posterior lines.
- Decide whether itâ€™s good or bad.
- Save onion images into GOOD_ONION_DB and BAD_ONION_DB.
